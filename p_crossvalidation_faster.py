# -*- coding: utf-8 -*-
"""p_CrossValidation_faster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iV-7u8rR5klnBJbxE71dylqdu8wBxzZS
"""

from google.colab import drive 
drive.mount('/content/drive')

import os
from collections import OrderedDict
import json
import cv2
import torch
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt


IMAGE_DIR = "/content/drive/My Drive/new_imgs"
ANNOTATION_DIR = "/content/drive/My Drive/whole_labels"

# making whole json_file

p = 'whole_patches'
os.makedirs(p, exist_ok=True)

pa = 'whole_patch_coco_octa.json'
empty_dict = OrderedDict()
with open(pa, "w", encoding='utf-8') as make_file:
  json.dump(empty_dict, make_file, indent='\t')

# make test patches & make test patch coco json

in_path = "/content/whole_coco_OCTA.json"
out_path = "/content/whole_patch_coco_octa.json"

# open existing json
with open (in_path, encoding='utf-8') as val_idata_file:
  val_indata = json.load(val_idata_file)
# open new json
with open (out_path, encoding='utf-8') as val_odata_file:
  val_outdata = json.load(val_odata_file)

# component of outdata
data = {}
data["license"] = []
data["images"] = []
data["annotations"] = []
# later add about info 

for i in range(len(val_indata["license"])):
  data["license"].append(val_indata["license"][i])
data["categories"] = val_indata["categories"]
data["info"] = val_indata["info"]

o = 1

for (path, dir, files) in os.walk(IMAGE_DIR):
  for i, filename in enumerate(files):
    # making patches using trainset
    im_path = os.path.join(IMAGE_DIR, filename)         
    img = cv2.imread(im_path, cv2.IMREAD_COLOR)
    img = torch.from_numpy(img)
    img = img.permute(2,0,1)
    patches = img.unfold(0,3,3).unfold(1,76,76).unfold(2,76,76)
    patches = patches.reshape(-1, 3, 76, 76) 
    patches = patches.numpy()
    # (16, 3, 76, 76)

    # making resized patches, default scale up 10
    for j in range(len(patches)):     
      patch_img = patches[j,:,:,:]
      patch_img = patch_img.transpose(1,2,0)
      resize = len(patch_img[0]) * 10
      resized_img = cv2.resize(patch_img, dsize=(resize, resize), interpolation=cv2.INTER_AREA)
      re = Image.fromarray(resized_img.astype(np.uint8))

      # save resized patch in folder
      save_name = filename+"_"+str(i)+"_"+str(j)+".png"
      path_save_name = "whole_patches/" + save_name 
      re.save(path_save_name)

      # images info for coco
      info_patch = {}
      info_patch["data_captured_file"] = filename
      info_patch["height"] = resize
      info_patch["width"] = resize
      info_patch["license"] = "no"
      info_patch["id"] = 16*i + (j+1)
      info_patch["file_name"] = save_name

      data["images"].append(info_patch)

      # annotation info for coco
      temp = 0

      # find matched image
      for m in range(len(val_indata["images"])):
        if (val_indata["images"][m]["file_name"] == filename):
          temp = val_indata["images"][m]["id"]
          break
      
      # find matched annotation with id
      matched_anno = []
      for p in range(len(val_indata["annotations"])):
        if (val_indata["annotations"][p]["image_id"] == temp):
          matched_anno.append(val_indata["annotations"][p]["bbox"])
      
      # match each detection boundary with patches
      for b in range(len(matched_anno)):
        anno_patch = {}
        lx, dy, w, h = matched_anno[b]
        rx = lx + w
        uy = dy - h
        x1, y1, x2, y2 = 76 * (j%4), 76 * (j//4), 76* (j%4 + 1), 76*(j//4 + 1)
        box = None

        if ( (lx < x1) & (x1 < rx) ):
          if ( (uy < y1) & (y1 < dy) ):
            box = [x1, y1, rx - x1, dy - y1]
          elif ( (y1 < uy) & (dy < y2) ):
            box = [x1, uy, rx - x1, h]
          elif ( (uy < y2) & (y2 < dy) ):
            box = [x1, uy, rx - x1, y2 - uy]
        
        elif (x1 < lx) and (rx < x2):
          if ( (uy < y1) & (y1 < dy) ):
            box = [lx, y1, w, dy - y1]
          elif ( (y1 < uy) & (dy < y2) ):
            box = [lx, uy, w, h]
          elif ( (uy < y2) & (y2 < dy) ):
            box = [lx, uy, w, y2 - uy]
        
        elif (lx < x2) and (x2 < rx):
          if ( (uy < y1) & (y1 < dy) ):
            box = [lx, y1, x2 - lx, dy - y1]
          elif ( (y1 < uy) & (dy < y2) ):
            box = [lx, uy, x2 - lx, h]
          elif ( (uy < y2) & (y2 < dy) ):
            box = [lx, uy, x2 - lx, y2 - uy]
        else:
          box = None
        
        if box is None:
          continue

        # gain all feature to 0 ~ 76, and scale up
        box[0] = box[0] - 76 * (j%4)
        box[1] = box[1] - 76 * (j//4)

        box[0] = box[0] * 10
        box[1] = box[1] * 10
        box[2] = box[2] * 10
        box[3] = box[3] * 10

        anno_patch["area"] = 0
        anno_patch["iscrowd"] = 0
        anno_patch["category_id"] = 1
        anno_patch["id"] = o
        o += 1
        anno_patch["image_id"] = 16*i + (j+1)
        anno_patch["bbox"] = box

        data["annotations"].append(anno_patch)

with open(out_path, 'w',encoding='utf-8') as val_odata_file:
  json.dump(data, val_odata_file, indent="\t")

print("done")

whole_file = "whole_patch_coco_octa.json"

with open (whole_file, encoding='utf-8') as dd_file:
  whole_data = json.load(dd_file)

# 20 20 25
# 0 320
# 320 640
# 640 1040

import copy
from copy import deepcopy

for m in range(3):
  start = 320 * m
  end = 320 * (m+1) if m < 2 else 1040 

  # for train_patch
  train_files = {}
  train_files = deepcopy(whole_data)
  train_files["images"].clear()
  for i in range(start, end):
    train_files["images"].append(whole_data["images"][i])
  print(len(train_files["images"]))

  # for val_patch
  val_files = {}
  val_files = deepcopy(whole_data)
  for j in range(start, end):
    del val_files["images"][start]
  print(len(val_files["images"]))

  # making each json
  tt = "train_patch_" + str(m) + ".json"
  vv = "val_patch_" + str(m) + ".json"

  with open(tt, "w+t") as ttt:
    json.dump(train_files, ttt, indent='\t')  
  with open(vv, "w+t") as vvv:
    json.dump(val_files, vvv, indent='\t')

# install dependencies: 
!pip install pyyaml==5.1 pycocotools>=2.0.1
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version
# opencv is pre-installed on colab

# install detectron2: (Colab has CUDA 10.1 + torch 1.6)
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
assert torch.__version__.startswith("1.6")
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

from detectron2.data.datasets import register_coco_instances

# register trainset
register_coco_instances("dataset_train0", {}, "train_patch_0.json", "/content/whole_patches")
register_coco_instances("dataset_val0", {}, "val_patch_0.json", "/content/whole_patches")

register_coco_instances("dataset_train1", {}, "train_patch_1.json", "/content/whole_patches")
register_coco_instances("dataset_val1", {}, "val_patch_1.json", "/content/whole_patches")

register_coco_instances("dataset_train2", {}, "train_patch_2.json", "/content/whole_patches")
register_coco_instances("dataset_val2", {}, "val_patch_2.json", "/content/whole_patches")

print("done")

"""### no need to see

### First Cross_val
"""

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("dataset_val0",)
cfg.DATASETS.TEST = ("dataset_train0",)
cfg.DATALOADER.NUM_WORKERS = 1
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.00025

cfg.SOLVER.MAX_ITER = 1000 #adjust up if val mAP is still rising, adjust down if overfit
cfg.SOLVER.GAMMA = 0.05

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2

# cfg.TEST.EVAL_PERIOD = 500

print("done")

from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

print(202)

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model
predictor = DefaultPredictor(cfg)
test_metadata = MetadataCatalog.get("dataset_train0",)
print("done")

from detectron2.utils.visualizer import ColorMode
import glob
import copy

whole = dict()
papa = "/content/train_patch_0.json"
with open (papa, encoding='utf-8') as test_data:
  ddd = json.load(test_data)

name_list = []
for images in ddd["images"]:
  k = "/content/whole_patches/" + images["file_name"]
  name_list.append(k)

# 320 patches
print(len(name_list))
ap = 1
 
for imageName in glob.glob('/content/whole_patches/*png'):
  if (imageName in name_list):
    im = cv2.imread(imageName)
    outputs = predictor(im)
    # v = Visualizer(im[:, :, ::-1],
    #               metadata=test_metadata, 
    #               scale=0.5
    #               )
    # out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    # cv2_imshow(out.get_image()[:, :, ::-1])
    print(ap)
    ap += 1

    original_box = []
    print(imageName)

    a = imageName.split(".")
    # print(a)
    n = a[0]
    n = n.split('/')[-1]
    key = n + ".png"
    # print(key)

    b = a[-2]
    b = b.split("_")[-1]
    b = int(b)
    # print(b)

    # original_box = []
    sa = len(outputs["instances"]._fields["pred_boxes"])
    if (sa == 0):
      continue
    for box in outputs["instances"]._fields["pred_boxes"]:
      box = box.to('cpu')
      box = box.numpy()
      box = box / 10
      box[0] = box[0] + (b%4) * 76
      box[2] = box[2] + (b%4) * 76
      box[1] = box[1] + (b//4) * 76
      box[3] = box[3] + (b//4) * 76
      box = box.tolist()

      s = outputs["instances"]._fields["scores"][sa-1]
      s = s.to('cpu')
      s = s.numpy()
      s = s.tolist()
      box.append(s)

      # print(box)
      # original_box.append(box)

      if (key not in whole.keys()):
        whole[key] = [box]
      else:
        whole[key].append(box)
    
    
print(whole)
print(len(whole))

for i in whole.keys():
  f = open(os.path.join("/content/drive/My Drive/f_0", i) + '.txt' , 'w')
  a = whole[i]
  for dataa in a:
    lx, ly, rx, ry, conf = dataa
    mmm = "aneurysm " + str(round(conf, 4)) + ' ' + str(round(lx)) + ' ' + str(round(ly)) + ' ' + str(round(rx)) + ' ' + str(round(ry))
    f.write(mmm)
    f.write('\n')
  f.close()

import matplotlib.pyplot as plt
print(len(whole.keys()))

for (path, dir, files) in os.walk("/content/drive/My Drive/new_imgs"):
  for file in files:
    if (file in whole.keys()):
      im_path = "/content/drive/My Drive/new_imgs/" + file

      img = cv2.imread(im_path, cv2.IMREAD_COLOR)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      for i in whole[file]:
        lx, ly, rx, ry, conf = i
        cv2.rectangle(img, (round(lx), round(ly)), (round(rx), round(ry)), (0, 0, 255), 1)

      re = Image.fromarray(img.astype(np.uint8))
      # save results
      mm = im_path.split("/")[-1]
      path_save_name = "/content/drive/My Drive/patch_faster/" + mm
      re.save(path_save_name)
      plt.imshow(img)
      print(im_path)

# making new

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.DATASETS.TEST = ("dataset_train0", )
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model
predictor = DefaultPredictor(cfg)
test_metadata = MetadataCatalog.get("dataset_train0",)

from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader
from detectron2.evaluation import COCOEvaluator, inference_on_dataset


evaluator = COCOEvaluator("dataset_train0", cfg, False, output_dir="./output/")
val_loader = build_detection_test_loader(cfg, "dataset_train0")
inference_on_dataset(trainer.model, val_loader, evaluator)

print(2020)

"""### Second cross_val"""

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("dataset_val1",)
cfg.DATASETS.TEST = ("dataset_train1",)
cfg.DATALOADER.NUM_WORKERS = 1
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.00025

cfg.SOLVER.MAX_ITER = 1000 #adjust up if val mAP is still rising, adjust down if overfit
cfg.SOLVER.GAMMA = 0.05

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2

cfg.TEST.EVAL_PERIOD = 500

print("done")

from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set the testing threshold for this model
predictor = DefaultPredictor(cfg)
test_metadata = MetadataCatalog.get("dataset_train1",)

from detectron2.utils.visualizer import ColorMode
import glob
import copy

whole = dict()
papa = "/content/train_patch_1.json"
with open (papa, encoding='utf-8') as test_data:
  ddd = json.load(test_data)

name_list = []
for images in ddd["images"]:
  k = "/content/whole_patches/" + images["file_name"]
  name_list.append(k)

# 320 patches
print(len(name_list))
ap = 1
 
for imageName in glob.glob('/content/whole_patches/*png'):
  if (imageName in name_list):
    im = cv2.imread(imageName)
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                  metadata=test_metadata, 
                  scale=0.5
                  )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])
    print(ap)
    ap += 1

    original_box = []
    print(imageName)

    a = imageName.split(".")
    # print(a)
    n = a[0]
    n = n.split('/')[-1]
    key = n + ".png"
    # print(key)

    b = a[-2]
    b = b.split("_")[-1]
    b = int(b)
    # print(b)

    # original_box = []
    for box in outputs["instances"]._fields["pred_boxes"]:
      # print(box)
      box = box.to('cpu')
      box = box.numpy()
      box = box / 10
      box[0] = box[0] + (b%4) * 76
      box[2] = box[2] + (b%4) * 76
      box[1] = box[1] + (b//4) * 76
      box[3] = box[3] + (b//4) * 76
      box = box.tolist()
      # print(box)
      # original_box.append(box)

      if (key not in whole.keys()):
        whole[key] = [box]
      else:
        whole[key].append(box)
    
print(whole)
print(len(whole))

# aneurysm
import matplotlib.pyplot as plt
print(len(whole.keys()))

for (path, dir, files) in os.walk("/content/drive/My Drive/imgs"):
  for file in files:
    if (file in whole.keys()):
      im_path = "/content/drive/My Drive/imgs/" + file

      img = cv2.imread(im_path, cv2.IMREAD_COLOR)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      for i in whole[file]:
        lx, ly, rx, ry = i
        cv2.rectangle(img, (round(lx), round(ly)), (round(rx), round(ry)), (0, 0, 255), 1)

      re = Image.fromarray(img.astype(np.float32))
      # save results
      mm = im_path.split("/")[-1]
      path_save_name = "/content/drive/My Drive/sooo/" + mm
      re.save(path_save_name)
      plt.imshow(img)
      print(im_path)

"""### Third cross_val"""

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("dataset_val2",)
cfg.DATASETS.TEST = ("dataset_train2",)
cfg.DATALOADER.NUM_WORKERS = 1
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.00025

cfg.SOLVER.MAX_ITER = 1000 #adjust up if val mAP is still rising, adjust down if overfit
cfg.SOLVER.GAMMA = 0.05

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2

cfg.TEST.EVAL_PERIOD = 500

print("done")

from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.95   # set the testing threshold for this model
predictor = DefaultPredictor(cfg)
test_metadata = MetadataCatalog.get("dataset_train2",)

from detectron2.utils.visualizer import ColorMode
import glob
import copy

whole = dict()
papa = "/content/train_patch_2.json"
with open (papa, encoding='utf-8') as test_data:
  ddd = json.load(test_data)

name_list = []
for images in ddd["images"]:
  k = "/content/whole_patches/" + images["file_name"]
  name_list.append(k)

# 320 patches
print(len(name_list))
ap = 1
 
for imageName in glob.glob('/content/whole_patches/*png'):
  if (imageName in name_list):
    im = cv2.imread(imageName)
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                  metadata=test_metadata, 
                  scale=0.5
                  )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])
    print(ap)
    ap += 1



    original_box = []
    print(imageName)

    a = imageName.split(".")
    # print(a)
    n = a[0]
    n = n.split('/')[-1]
    key = n + ".png"
    # print(key)

    b = a[-2]
    b = b.split("_")[-1]
    b = int(b)
    # print(b)

    # original_box = []
    for box in outputs["instances"]._fields["pred_boxes"]:
      # print(box)
      box = box.to('cpu')
      box = box.numpy()
      box = box / 10
      box[0] = box[0] + (b%4) * 76
      box[2] = box[2] + (b%4) * 76
      box[1] = box[1] + (b//4) * 76
      box[3] = box[3] + (b//4) * 76
      box = box.tolist()
      # print(box)
      # original_box.append(box)

      if (key not in whole.keys()):
        whole[key] = [box]
      else:
        whole[key].append(box)
    
print(whole)
print(len(whole))

import matplotlib.pyplot as plt
print(len(whole.keys()))

for (path, dir, files) in os.walk("/content/drive/My Drive/imgs"):
  for file in files:
    if (file in whole.keys()):
      im_path = "/content/drive/My Drive/imgs/" + file

      img = cv2.imread(im_path, cv2.IMREAD_COLOR)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      for i in whole[file]:
        lx, ly, rx, ry = i
        cv2.rectangle(img, (round(lx), round(ly)), (round(rx), round(ry)), (0, 0, 255), 1)

      re = Image.fromarray(img.astype(np.uint8))
      # save results
      mm = im_path.split("/")[-1]
      path_save_name = "/content/drive/My Drive/soooo/" + mm
      re.save(path_save_name)
      plt.imshow(img)
      print(im_path)

img_path = "/content/drive/My Drive/soooo/22935176_3mm_DCP_OS_20180214.png"
/content/drive/My Drive/soooo/26883105_3mm_SCP_OS_20180831.png
/content/drive/My Drive/soooo/26840553_3mm_Retina_OS_20180123.png
/content/drive/My Drive/soooo/26840553_3mm_Retina_OD_20190117.png
/content/drive/My Drive/soooo/26840553_3mm_DCP_OD_20190117.png
/content/drive/My Drive/soooo/26840553_3mm_DCP_OD_20180412.png

img = cv2.imread(im_path, cv2.IMREAD_COLOR)
lxl = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
/content/drive/My Drive/sooo/21910307_3mm_DCP_OD_20190219.png